{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appreciated-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2] elementwise multiplication\n",
      "[3 2 3] elementwise addition\n",
      "Error:[0.00053374] Prediction:-0.0026256193329783125\n"
     ]
    }
   ],
   "source": [
    "# You can train neural networks to convert a given dataset of \"what you know\" to a dataset of \"what you want to know\"\n",
    "# Basically, you can train the network to interpret observations.\n",
    "# First, convert the observation dataset into matrices so the information is interpretable for the network.\n",
    "    # Convention: use one row for one observation (each set of on/off lights on a 3-light streetlight) \n",
    "    # and one column per observed item (whether each light in the set is on or off). \n",
    "    # Ideally, you want a \"lossless representation\" - the data and the matrix can be perfectly converted between each other.\n",
    "    \n",
    "import numpy as np\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "# input data pattern\n",
    "# 0 = light is off, 1 = light is on in a 3-light horizontal stoplight at a crosswalk\n",
    "streetlights = np.array([[1, 0, 1],\n",
    "                       [0, 1, 1],\n",
    "                       [0, 0, 1],\n",
    "                       [1, 1, 1],\n",
    "                       [0, 1, 1],\n",
    "                       [1, 0, 1]])\n",
    "# output data pattern \n",
    "# 0 = stop, 1 = walk\n",
    "walk_vs_stop = np.array([[0],\n",
    "                        [1],\n",
    "                        [0],\n",
    "                        [1],\n",
    "                        [1],\n",
    "                        [0]])\n",
    "\n",
    "# First, we can turn streetlights into walk_vs_stop with a neural network, as before.\n",
    "# Uses nice numpy arrays to do elementwise addition/multiplication easily, otherwise is same as previous neural networks.\n",
    "print(streetlights[0] * [2, 2, 2], \"elementwise multiplication\")\n",
    "print(streetlights[0] + [2, 2, 2], \"elementwise addition\")\n",
    "\n",
    "for iteration in range(40):\n",
    "    error_for_all_lights = 0\n",
    "    for row in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row]\n",
    "        goal_prediction = walk_vs_stop[row]\n",
    "        \n",
    "        # dot product = weighted sum: input * weights and addition of all items in vector to return a single number\n",
    "        # The weighted sum of inputs finds perfect correlation between input and output by weighting decorrelated inputs to 0.\n",
    "        # Basically, if the light is off (marked 0), it will have no effect on the outcome because 0 * anything = 0.\n",
    "        # So anytime a light has an effect, it will not be 0 and it will be accounted for as a value that affects the outcome.\n",
    "        prediction = input.dot(weights) \n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_for_all_lights += error\n",
    "        \n",
    "        delta = prediction - goal_prediction \n",
    "        weights = weights - (alpha * (input * delta))\n",
    "\n",
    "print(\"Error:\" + str(error_for_all_lights) + \" Prediction:\" + str(prediction))\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "# The network goes through the training examples one at a time and iterates over it several times. This lets it update the \n",
    "# weights for all examples until the network is capable of predicting the correct answer when faced with all training examples.\n",
    "# This was essentially what we did in ch5 to train the handwriting neural network. We did not have a separate error for the\n",
    "# entire dataset, however. We just updated the error for each digit 0-9 and used that error for all instances of that digit\n",
    "# in the dataset. This doesn't seem to make a difference for the network's learning because we don't actually use the error \n",
    "# value to learn. We use delta, which is just kind of related.\n",
    "\n",
    "# (Average/Full) Gradient Descent\n",
    "# The network goes through the entire set of training examples and calculates the average weight_delta for the whole dataset.\n",
    "# Then, the network changes the weights one time. The network does not change the weights for every data point.\n",
    "\n",
    "# Batch Gradient Descent\n",
    "# Updates the weights after n data points. Batch size is chosen by the user and is typically between 8 and 256. This will be\n",
    "# discussed more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting\n",
    "# There is an edge case where the network will predict the right answer but not actually learn. For example, what if the left\n",
    "# and right weights were 0.5 and -0.5 respectively and our data point was [1, 0, 1]? Then the weighted sum (prediction) would \n",
    "# be 0. The prediction was correct (stop), but the network did not learn anything.\n",
    "\n",
    "# Error is shared among all weights. If some weight configuration accidentally creates perfect correlation between the \n",
    "# prediction and the expected output (error = 0), then weights will not be updated properly and the network will not learn from\n",
    "# this data point.\n",
    "\n",
    "# Overfitting is really only a problem if you only train on data points that the network cannot learn off of. The other data\n",
    "# points should bump the weights out of this configuration and you can continue learning as long as you see other data points.\n",
    "\n",
    "# Networks should be exposed to plenty of data in order to make sure they learn the rule. They need to learn to generalize \n",
    "# instead of memorizing some specific examples and reacting accordingly.\n",
    "\n",
    "# Conflicting Pressure\n",
    "# Notice in our stoplight example, the third light is always on. How does the network know that it has to bring the weight down\n",
    "# to 0 for walking even though there are both positive and negative pressures exerted on the weight for this light?\n",
    "# There is something called \"regularization\" that forces weights with conflicting pressure to move to 0, we will discuss later.\n",
    "# A weight with conflicting pressure doesn't really do anything except confuse, so it makes sense to silence it.\n",
    "# With regularization, you can learn that the third light is useless more quickly than without. If you don't have regularization,\n",
    "# you can still learn that the light is useless, but it won't happen until the first light (for stopping) and the second light\n",
    "# (for walking) have already settled into their perfectly correlated weights.\n",
    "# If the correlations weren't perfect, the network might have struggled to silence the unnecessary third weight. Regularization\n",
    "# can help avoid that problem.\n",
    "\n",
    "# If the network is given a dataset where the input has no correlation with the output (all weights have conflicting pressure),\n",
    "# it won't be able to solve anything. In this case, you can create \"intermediate data\" in order to predict the output. You can\n",
    "# do this by feeding the input to a network which then produces results (intermediate data/output layer 1). You can then put\n",
    "# these results into another network and this network will be able to use the intermediate data to predict the output (output\n",
    "# layer 2). \n",
    "\n",
    "# \"Because the input dataset doesn't correlate with the output dataset, you'll use the input dataset to create an intermediate\n",
    "# dataset that DOES have correlation with the output. It's kind of like cheating.\"\n",
    "\n",
    "# So how do you figure out what the delta (normalized error) values are in the first network which takes the lights and outputs\n",
    "# some data? The second network is the same as stuff we have been doing; it just takes the output from network 1 and gets\n",
    "# trained to output a prediction of walk or run. But the first network takes lights and has to output something. What is the \n",
    "# delta value there?? How do we know we are outputting the correct thing when we're just making up outputs?\n",
    "\n",
    "# Turns out that the weights for the first network directly cause the second network's prediction (obviously, since we made\n",
    "# this data from network 1's weights and we are using it as input for network 2). So the weights of network 1 directly influence\n",
    "# error of network 2. You can use delta from network 2 to figure out the delta of network 1. Just multiply the delta from \n",
    "# network 2 (only 1 value because it is the normalized error for this one prediction) by all of the network 1 weights. This\n",
    "# moves the delta back to network 1 from network 2 and this is called \"backpropagation.\"\n",
    "\n",
    "# Backpropagation\n",
    "# Remember that delta tells you the direction and amount we have to adjust things to get the right answer. For example, if we get\n",
    "# a result that is too low, we have a delta that will be a positive number because we want to raise the result. And this delta\n",
    "# will really mean \"if we want to result to be raised by x amount, we have to raise/lower all of these inputs into network 2 by\n",
    "# some amount\" and the inputs that go into network 2 come directly from network 1 and they are produced by the weights of \n",
    "# network 1. So network 1's weights have to be multiplied by some delta value. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
